{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ve started a data science project to understand the connection between natural disasters and popular opinion, especially towards the government and other aid providers. I plan on applying sentiment analysis tools to Twitter data after natural disasters, beginning with the 2018 Anchorage Earthquake because the tweets will be in English and reference organizations with which I should be nominally familiar. While I have practically zero experience in these types projects, I do have… ummm... grit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... more importantly, I have the Internet and it's vast quantities of information, some of which are in the form of MOOCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My knowledge of programming is entirely derived from:  \n",
    "  \n",
    "    -CodeAcademy [Python, Git]  \n",
    "    -\"Natural Language Processing with Python\"  \n",
    "    -Coursera's Machine Learning course with Andrew Ng  \n",
    "    -\"Python for Data Analysis\"  \n",
    "    -Fastai's Machine Learning for Coders  \n",
    "    -UTAustin's MOOC: Foundations of Data Analysis pt. I  \n",
    "    -Stanford's CS224n  \n",
    "    -Random articles about Sentiment Analysis courtesy of Google  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My method for studying natural disasters and popular opinion was sentiment analysis of publically available responses in the wake of natural disasters. Specifically, I focused on Tweet data because guides for scraping the information were plentiful and I believed the volume would lend itself to consistent results.\n",
    "\n",
    "I trained a variety of models, some shallow, some deeper. I used TF-IDF and CountVectorizer, although by the time I finish writing I hope to have implemented word2vec as well. To be honest, I did not completely grasp what I was doing as I used them, but my understand has been deepend by the process of writing down my thoughts and evaluating them after some time away. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Search Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To identify key search terms, I essentially googled 'Alaska Earthquake' (sans quotation marks) and looked at the first couple pages of results. I wrote down repeated words and phrases, hashtags, organizations. I eventually settled on: \"Alaska Earthquake\", \"AKQuake\", and \"AlaskaEarthquake\". More detailed findings are here: https://medium.com/@chylirk/disaster-project-report-1-1708b838bbf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To scrape the tweets, I initially used Tweepy (the native Twitter API) and Selenium. I settled on the latter because it provided an order of magnitude more tweets than Tweepy, even using the same search terms. The Alaska Earthquake has largely disappeared from public discourse which could account for the paucity of Tweepy results. From the little I've read, Tweepy's better suited for streaming data.\n",
    "\n",
    "(I'd like to give a big shout out to Amardeep Chauhan and this exquisitely clear guide to using Selenium - https://towardsdatascience.com/selenium-tweepy-to-scrap-tweets-from-tweeter-and-analysing-sentiments-1804db3478ac)\n",
    "\n",
    "(And another nod of gratitude to Marco Bonzanini for their posts about Tweepy - The book on Social Media and Python is on the 'To Read' list - https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I wish I had taken UTAustin's Fundamentals of Data Analysis course prior to exploration. I needed the reps in basic visualization and regression with R more than I needed half-remembered sections of 'Python for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I made wordclouds from the various tweet results, split them into earlier and later. I created scatterplots of likes and retweets over time. They suggested avenues for inquiry. I wrote more about them here - https://medium.com/@chylirk/disaster-project-report-2-9adf2691be91  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Labeling was entirely by hand with sentiment values of (-1, 0, 1). I didn't finish in one shot, nor did I keep track closely. Assuming each label took approxamitely 2 seconds, I estimate labelling took an hour all told."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Digression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This entire coding experience has opened my eyes to possibilities of generosity. I realize now that my outlook on careers and academic advancement was extremely corporate, largely about identifying scarcities, securing competitive advantages, and rendering them defensible. My experiences with MOOCs, online guides, and especially fastai have really denaturalized that perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My benchmark was Textblob which returned an accuracy score of 50%  \n",
    "    \n",
    "Next I used a Logistic Regression with CountVectorizer - I learned about this through fast.ai's Machine learning for Coders course in addition to this wonderfully detailed Kaggle kernel - https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression's peak accuracy was ~80%, a substantial improvement over textblob, although I was surprised to see that unigram features alone consistently outperformed models that included bigrams and trigrams - For this part of the project, I'm indebted to Ricky Kim's series of blogposts at TowardsDataScience [https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, I compared CountVectorizer with TF-IDF, only to find that CountVectorizer-based models consistently outperformed TF-IDF ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following these experiments solely with Logistic Regression, I then compared Logistic Regression to other algorithms:  \n",
    "* LinearSVC,  \n",
    "* AdaBoostClassifier,  \n",
    "* MultinomialNB,  \n",
    "* BernoulliNB,  \n",
    "* RidgeClassifier,  \n",
    "* PassiveAggressiveClassifier,  \n",
    "* Perceptron,  \n",
    "* and NearestCentroid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
